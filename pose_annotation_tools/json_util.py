import numpy as np
import os,sys
import pandas as pd
from PIL import Image
import json
from scipy.spatial.distance import cdist, euclidean
import copy
import pickle
import yaml


def count_workers(data):
    nWorkers = 0
    for f,frame in enumerate(data):
        if ('annotatedResult' in frame.keys()): #check if this frame has at least one set of annotations
            nWorkers = max(nWorkers,len(frame['annotatedResult']['annotationsFromAllWorkers']))
    return nWorkers


def apply_flip_correction(frame, meds, keypoints, pair):
    if not pair[0] in keypoints or not pair[1] in keypoints:
        raise SystemExit('annot_config error: one or more entries of check_pairs is not a member of keypoints. Please check project_config.yml')

    i1 = keypoints.index(pair[0])
    i2 = keypoints.index(pair[1])

    for rep in range(3): #repeat 3 times for stability
        for w,worker in enumerate(frame.swapaxes(0,2).swapaxes(1,2)):
            d1 = cdist(worker[[i1,i2],:],[meds[:,i1]])
            d2 = cdist(worker[[i1,i2],:],[meds[:,i2]])
            if (d1[0]>d2[1]) and (d2[1]>d1[0]):
                frame[[i1, i2],:,w] = frame[[i2, i1],:,w]

        # re-compute the medians:
        meds = np.median(np.squeeze(frame.T),axis=0)

    return frame, meds


def manifest_to_dict(manifest_file, im_path, save_file, config):
    """
    Converts an annotation file generated by AMT/Ground Truth into the MARS json format.
    """

    animal_names   = config['animal_names'] if config['animal_names'] else config['species']
    species        = config['species']
    keypoint_names = config['keypoints']
    check_pairs    = config['check_pairs']
    nKpts          = len(keypoint_names)

    fid = open(manifest_file, 'r')
    data = []
    for line in fid:
        data.append(json.loads(line))
    nWorkers = count_workers(data)

# replace this with a more general formulation of the frame_dict?------------------------------------------------------
    # loop over frames in the manifest file
    errors = [] # unused?
    D = []
    for f, sample in enumerate(data):
        # Use the path to the image data to open the image.
        im = Image.open(im_path + data[f]['source-ref'].split('/')[-1])
        im = (np.asarray(im)).astype(float)

        # extract info for each worker
        XB, YB, XW, YW = ([] for i in range(4))
        for w, worker in enumerate(sample['annotatedResult']['annotationsFromAllWorkers']):
            # the json of annotations from each worker is stored as a string for security reasons.
            # we'll use eval to convert it into a dict:
            points_dict = eval(worker['annotationData']['content'])

            black_mouse_x = [0] * nKpts
            black_mouse_y = [0] * nKpts
            white_mouse_x = [0] * nKpts
            white_mouse_y = [0] * nKpts
            for pt in points_dict[manifest_key]['keypoints']:
                mouse = 'black' if 'black' in pt['label'] else 'white'
                idx = keypoint_names.index(pt['label'].replace(mouse ,'').replace('mouse' ,'').strip())

                if mouse == 'black':
                    black_mouse_x[idx] = pt['x']/im.shape[1]
                    black_mouse_y[idx] = pt['y']/im.shape[0]
                else:
                    white_mouse_x[idx] = pt['x']/im.shape[1]
                    white_mouse_y[idx] = pt['y']/im.shape[0]
            XB.append(black_mouse_x)
            YB.append(black_mouse_y)
            XW.append(white_mouse_x)
            YW.append(white_mouse_y)

        # Compute some statistics for the tfrecord and append.
        frame_dict = make_frame_dict(XB, YB, XW, YW, keypoint_names, im.shape, sample['source-ref'].split('/')[-1])
        D.append(frame_dict)
#---------------------------------------------------------------------------------------------------------------------------

    # save info to file
    with open(save_file, 'w') as fp:
        json.dump(D,fp)


def csv_to_dict(csv_file, im_path, save_file, config=[]):
    """
    Converts manual annotations created by DeepLabCut from csv into the format created by MARS from
    AMT or Ground Truth manifest files. MARS can then use that json to create tfrecords for training
    the detection and pose models.
    """
    with open(csv_file) as datafile:
        next(datafile)
        if "individuals" in next(datafile):
            header = list(range(4))
            multianimal = True
        else:
            header = list(range(3))
            multianimal = False
    data = pd.read_csv(csv_file, index_col=0, header=header)

    worker_names = list(set(list(data.columns.get_level_values(0))))  # find all the annotators.

    if config:
        keypoint_names = config['keypoints']
    else:
        keypoint_names = list(data.columns.get_level_values(1))      # find all the keypoints.
        keypoint_names = list(OrderedDict.fromkeys(keypoint_names))  # get rid of redundancies while preserving order.

    nKpts = len(keypoint_names)

    # loop over frames in the manifest file
    errors = []
    D = []
    for f, image in enumerate(data.index):

        # Use the path to the image data to open the image.
        im = Image.open(im_path + os.path.split(str(image))[-1])
        im = (np.asarray(im)).astype(float)

        # convert our dataframe to a dict
        annot = data.loc[image]
        # TODO: update this to handle the multianimal case.
        annot = {level0: {level1: annot.xs([level0, level1]).to_dict() for level1 in annot.index.levels[1]}
                 for level0 in annot.index.levels[0]}

        # extract info for each worker
        XB, YB, XW, YW = ([] for i in range(4))
        for w, worker in annot.items():
            black_mouse_x = [0.] * nKpts
            black_mouse_y = [0.] * nKpts
            white_mouse_x = [0.] * nKpts
            white_mouse_y = [0.] * nKpts

            for label,pt in worker.items():
                # todo: get color if none was provided!
                mouse = 'white' if 'white' in label else 'black' # if no color was provided, assume black
                idx = keypoint_names.index(label.replace(mouse ,'').replace('mouse' ,'').strip())

                if mouse == 'black':
                    black_mouse_x[idx] = pt['x']/im.shape[1]
                    black_mouse_y[idx] = pt['y']/im.shape[0]
                else:
                    white_mouse_x[idx] = pt['x']/im.shape[1]
                    white_mouse_y[idx] = pt['y']/im.shape[0]

            XB.append(black_mouse_x)
            YB.append(black_mouse_y)
            XW.append(white_mouse_x)
            YW.append(white_mouse_y)

        # Compute some statistics for the tfrecord and append.
        frame_dict = make_frame_dict(XB, YB, XW, YW, keypoint_names, im.shape, os.path.split(str(image))[-1])
        D.append(frame_dict)

    # save info to file
    with open(save_file, 'w') as fp:
        json.dump(D, fp)


def make_frame_dict(XB, YB, XW, YW, keypoint_names, im_shape, im_name):
    XB = np.array(XB)
    YB = np.array(YB)
    XW = np.array(XW)
    YW = np.array(YW)

    mXB = geometric_median(XB) if len(XB) > 1 else XB[0]
    mYB = geometric_median(YB) if len(YB) > 1 else YB[0]
    mXW = geometric_median(XW) if len(XW) > 1 else XW[0]
    mYW = geometric_median(YW) if len(YW) > 1 else YW[0]

    muXB = np.mean(XB, axis=0)
    muYB = np.mean(YB, axis=0)
    muXW = np.mean(XW, axis=0)
    muYW = np.mean(YW, axis=0)

    stdXB = np.std(YB, axis=0)
    stdYB = np.std(YB, axis=0)
    stdXW = np.std(XW, axis=0)
    stdYW = np.std(YW, axis=0)

    Bxmin = min(mXB)
    Bxmax = max(mXB)
    Bymin = min(mYB)
    Bymax = max(mYB)
    Wxmin = min(mXW)
    Wxmax = max(mXW)
    Wymin = min(mYW)
    Wymax = max(mYW)

    Bxmin, Bxmax, Bymin, Bymax = correct_box(Bxmin, Bxmax, Bymin, Bymax)
    Wxmin, Wxmax, Wymin, Wymax = correct_box(Wxmin, Wxmax, Wymin, Wymax)

    # area of bboxes
    Barea = abs(Bxmax - Bxmin) * abs(Bymax - Bymin) * im_shape[0] * im_shape[1]
    Warea = abs(Wxmax - Wxmin) * abs(Wymax - Wymin) * im_shape[0] * im_shape[1]

    # store info for Black and White mouse
    frame_dict = {
        'frame_name': im_name,
        'height': im_shape[0],
        'width': im_shape[1],
        'frame_id': im_name,
        'ann_label': keypoint_names,
        'ann': [],
        'ann_B': {'X': XB.tolist(),
                  'Y': YB.tolist(),
                  'bbox': np.array([Bxmin, Bxmax, Bymin, Bymax]).tolist(),
                  'med': np.array([mYB, mXB]).tolist(),
                  'mu': np.array([muYB, muXB]).tolist(),
                  'std': np.array([stdYB, stdXB]).tolist(),
                  'area': Barea.tolist(),
                  },
        'ann_W': {'X': XW.tolist(),
                  'Y': YW.tolist(),
                  'bbox': np.array([Wxmin, Wxmax, Wymin, Wymax]).tolist(),
                  'med': np.array([mYW, mXW]).tolist(),
                  'mu': np.array([muYW, muXW]).tolist(),
                  'std': np.array([stdYW, stdXW]).tolist(),
                  'area': Warea.tolist(),
                  }
                  }
    return frame_dict


def correct_box(xmin, xmax, ymin, ymax, stretch_const=0.04, stretch_factor=0.30, useConstant=True):
    # Code to modify bounding boxes to be a bit larger than the keypoints.

    if useConstant:
        stretch_constx = stretch_const
        stretch_consty = stretch_const
    else:
        stretch_constx = (xmax - xmin) * stretch_factor  # of the width
        stretch_consty = (ymax - ymin) * stretch_factor

    # Calculate the amount to stretch the x by.
    x_stretch = np.minimum(xmin, abs(1 - xmax))
    x_stretch = np.minimum(x_stretch, stretch_constx)

    # Calculate the amount to stretch the y by.
    y_stretch = np.minimum(ymin, abs(1 - ymax))
    y_stretch = np.minimum(y_stretch, stretch_consty)

    # Adjust the bounding box accordingly.
    xmin -= x_stretch
    xmax += x_stretch
    ymin -= y_stretch
    ymax += y_stretch
    return xmin,xmax,ymin,ymax
