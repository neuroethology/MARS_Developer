import numpy as np
import os,sys
import pandas as pd
from PIL import Image
import json
from scipy.spatial.distance import cdist, euclidean
from collections import OrderedDict
import copy
import pickle
import yaml


def count_workers(data):
    nWorkers = 0
    for f,frame in enumerate(data):
        if ('annotatedResult' in frame.keys()): #check if this frame has at least one set of annotations
            nWorkers = max(nWorkers,len(frame['annotatedResult']['annotationsFromAllWorkers']))
    return nWorkers


def apply_flip_correction(frame, meds, keypoints, pair):
    if not pair[0] in keypoints or not pair[1] in keypoints:
        raise SystemExit('annot_config error: one or more entries of check_pairs is not a member of keypoints. Please check project_config.yml')

    i1 = keypoints.index(pair[0])
    i2 = keypoints.index(pair[1])

    for rep in range(3):  # repeat 3 times for stability
        for w,worker in enumerate(frame.swapaxes(0,2).swapaxes(1, 2)):
            d1 = cdist(worker[[i1,i2], :], [meds[:, i1]])
            d2 = cdist(worker[[i1,i2], :], [meds[:, i2]])
            if (d1[0]>d2[1]) and (d2[1]>d1[0]):
                frame[[i1, i2],:,w] = frame[[i2, i1], :, w]

        # re-compute the medians:
        meds = np.median(np.squeeze(frame.T),axis=0)

    return frame, meds


def manifest_to_dict(project, correct_flips=True):
    """
    Converts an annotation file generated by AMT/Ground Truth into the MARS json format.
    """
    config_fid = os.path.join(project,'project_config.yml')
    with open(config_fid) as f:
        config = yaml.load(f)

    manifest_file  = config['manifest_name']
    save_file      = os.path.join(project,'annotation_data','processed_keypoint_annotations')
    animal_names   = config['animal_names'] if config['animal_names'] else config['species']
    species        = config['species']
    keypoint_names = config['keypoints']
    check_pairs    = config['check_pairs']
    nKpts          = len(keypoint_names)

    fid = open(os.path.join(project,'annotation_data',manifest_file), 'r')
    data = []
    for line in fid:
        data.append(json.loads(line))
    nWorkers = count_workers(data)
    nSamp = len(data)

    # loop over frames in the manifest file
    images = ['']*nSamp      # store local paths to labeled images
    hits = [False]*nSamp     # track which images have annotations (hopefully all of them)
    workerCount = [0]*nSamp  # track the number of workers who labeled each image

    sourceStr = os.path.dirname(data[0]['source-ref'])  # image path on AWS
    localStr = os.path.join(project, 'annotation_data', 'raw_images')  # image path locally

    D = []
    for f, sample in enumerate(data):
        if ('annotatedResult' in sample.keys()): #check if this frame has at least one set of annotations
            hits[f] = True
            images[f] = sample['source-ref']
            images[f].replace(sourceStr,localStr)

            # Use the path to the image data to open the image.
            im = Image.open(images[f])
            im = (np.asarray(im)).astype(float)
            rawPts = {n:np.empty((nKpts, 2)) for n in animal_names}
            for w,worker in enumerate(sample['annotatedResult']['annotationsFromAllWorkers']):
                workerCount[f] = workerCount[f] + 1

                # the json of annotations from each worker is stored as a string for security reasons.
                # we'll use eval to convert it into a dict:
                annot = eval(worker['annotationData']['content'])

                # now we can unpack this worker's annotations for each keypoint:
                for pt in annot['annotatedResult']['keypoints']:
                    animal = next((n for n in animal_names if n in pt['label']),config['species'])
                    part = keypoint_names.index(pt['label'].replace(animal, '').replace(species, '').strip())

                    rawPts[animal][part,0,w] = pt['x']/im.shape[1]
                    rawPts[animal][part,1,w] = pt['y']/im.shape[0]

            for id, animal in enumerate(animal_names):
                rawPts[animal] = rawPts[animal][:, :, :workerCount[f] + 1]  # remove missing annotators

                if correct_flips:  # adjust L/R assignments to try to find better median estimates.
                    meds = np.median(rawPts[animal], axis=2)
                    for pair in check_pairs:
                        rawPts[animal], meds = apply_flip_correction(rawPts[animal], meds, keypoint_names, pair)

                # Compute some statistics for the tfrecord and append.
                # TODO: update make_frame_dict so it takes rawPts[animal + animal + id as inputs!!!!-----------------------------------------------
                frame_dict = make_frame_dict(rawPts[animal], animal, id, keypoint_names, im.shape, sample['source-ref'].split('/')[-1])
                D.append(frame_dict)
# ----------------------------------------------------------------------------------------------------------------------

    # save info to file
    with open(save_file, 'w') as fp:
        json.dump(D,fp)


def csv_to_dict(csv_file, im_path, save_file, config=[]):
    """
    Converts manual annotations created by DeepLabCut from csv into the format created by MARS from
    AMT or Ground Truth manifest files. MARS can then use that json to create tfrecords for training
    the detection and pose models.
    """
    with open(csv_file) as datafile:
        next(datafile)
        if "individuals" in next(datafile):
            header = list(range(4))
            multianimal = True
        else:
            header = list(range(3))
            multianimal = False
    data = pd.read_csv(csv_file, index_col=0, header=header)

    worker_names = list(set(list(data.columns.get_level_values(0))))  # find all the annotators.

    if config:
        keypoint_names = config['keypoints']
    else:
        keypoint_names = list(data.columns.get_level_values(1))      # find all the keypoints.
        keypoint_names = list(OrderedDict.fromkeys(keypoint_names))  # get rid of redundancies while preserving order.

    nKpts = len(keypoint_names)

    # loop over frames in the manifest file
    errors = []
    D = []
    for f, image in enumerate(data.index):

        # Use the path to the image data to open the image.
        im = Image.open(im_path + os.path.split(str(image))[-1])
        im = (np.asarray(im)).astype(float)

        # convert our dataframe to a dict
        annot = data.loc[image]
        # TODO: update this to handle the multianimal case.
        annot = {level0: {level1: annot.xs([level0, level1]).to_dict() for level1 in annot.index.levels[1]}
                 for level0 in annot.index.levels[0]}

        # extract info for each worker
        XB, YB, XW, YW = ([] for i in range(4))
        for w, worker in annot.items():
            black_mouse_x = [0.] * nKpts
            black_mouse_y = [0.] * nKpts
            white_mouse_x = [0.] * nKpts
            white_mouse_y = [0.] * nKpts

            for label,pt in worker.items():
                # todo: get color if none was provided!
                mouse = 'white' if 'white' in label else 'black' # if no color was provided, assume black
                idx = keypoint_names.index(label.replace(mouse ,'').replace('mouse' ,'').strip())

                if mouse == 'black':
                    black_mouse_x[idx] = pt['x']/im.shape[1]
                    black_mouse_y[idx] = pt['y']/im.shape[0]
                else:
                    white_mouse_x[idx] = pt['x']/im.shape[1]
                    white_mouse_y[idx] = pt['y']/im.shape[0]

            XB.append(black_mouse_x)
            YB.append(black_mouse_y)
            XW.append(white_mouse_x)
            YW.append(white_mouse_y)

        # Compute some statistics for the tfrecord and append.
        frame_dict = make_frame_dict(XB, YB, XW, YW, keypoint_names, im.shape, os.path.split(str(image))[-1])
        D.append(frame_dict)

    # save info to file
    with open(save_file, 'w') as fp:
        json.dump(D, fp)


def make_frame_dict(X, Y, keypoint_names, im_shape, im_name):
    X = np.array(X)
    Y = np.array(Y)

    mX = np.median(X) if len(X) > 1 else X[0]
    mY = np.median(Y) if len(Y) > 1 else Y[0]

    muX = np.mean(X, axis=0)
    muY = np.mean(Y, axis=0)

    stdX = np.std(Y, axis=0)
    stdY = np.std(Y, axis=0)

    # bounding box
    Bxmin = min(mX)
    Bxmax = max(mX)
    Bymin = min(mY)
    Bymax = max(mY)
    Bxmin, Bxmax, Bymin, Bymax = correct_box(Bxmin, Bxmax, Bymin, Bymax)
    Barea = abs(Bxmax - Bxmin) * abs(Bymax - Bymin) * im_shape[0] * im_shape[1]

    # store info for this mouse
    frame_dict = {
        'frame_name': im_name,
        'height': im_shape[0],
        'width': im_shape[1],
        'frame_id': im_name,
        'ann_label': keypoint_names,
        'ann': [],
        'ann_B': {'X': X.tolist(),
                  'Y': Y.tolist(),
                  'bbox': np.array([Bxmin, Bxmax, Bymin, Bymax]).tolist(),
                  'med': np.array([mY, mX]).tolist(),
                  'mu': np.array([muY, muX]).tolist(),
                  'std': np.array([stdY, stdX]).tolist(),
                  'area': Barea.tolist(),
                  },
                  }
    return frame_dict


def correct_box(xmin, xmax, ymin, ymax, stretch_const=0.04, stretch_factor=0.30, useConstant=True):
    # Code to modify bounding boxes to be a bit larger than the keypoints.

    if useConstant:
        stretch_constx = stretch_const
        stretch_consty = stretch_const
    else:
        stretch_constx = (xmax - xmin) * stretch_factor  # of the width
        stretch_consty = (ymax - ymin) * stretch_factor

    # Calculate the amount to stretch the x by.
    x_stretch = np.minimum(xmin, abs(1 - xmax))
    x_stretch = np.minimum(x_stretch, stretch_constx)

    # Calculate the amount to stretch the y by.
    y_stretch = np.minimum(ymin, abs(1 - ymax))
    y_stretch = np.minimum(y_stretch, stretch_consty)

    # Adjust the bounding box accordingly.
    xmin -= x_stretch
    xmax += x_stretch
    ymin -= y_stretch
    ymax += y_stretch
    return xmin,xmax,ymin,ymax
