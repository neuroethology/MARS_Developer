{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_frames as ef\n",
    "import MARS_AWS_helpers as mah\n",
    "import MARS_annotation_tools as mat\n",
    "import time\n",
    "\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract video frames for annotation\n",
    "\n",
    "The function `extract_frames` will sample frames from all videos found in a directory, and save those frames as jpg files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing the videos we'd like to sample for training data:\n",
    "source_videos = '/path/to/videodir'\n",
    "\n",
    "# folder where we'll save the sampled images:\n",
    "image_directory = '/path/to/savedir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll extract a set of `nframes` frames from `source_videos` and save them into `target_folder`. We're going to skip the first 100 frames of each video, when the experimenter's hand might be in the field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nframes = 1000\n",
    "to_skip = 100\n",
    "\n",
    "ef.extract_frames(source_videos, image_directory, n_frames, to_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an annotation job on AWS\n",
    "\n",
    "You can find detailed instructions for setting up AWS to run annotation jobs [here](docs/readme_groundTruthSetup.md). For this notebook, we will assume you've set up your Lambdas, uploaded your images and the annotation template to a bucket, and created a manifest file for that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the s3 bucket containing the data to be annotated.\n",
    "BUCKET = 'white-100'\n",
    "\n",
    "# name of the data manifest inside that bucket.\n",
    "MANIFEST = 'white_mouse_100'\n",
    "\n",
    "# path to the template for the annotation interface.\n",
    "UITEMPLATE = 's3://white-mouse-test/final_keypoint_unique_label_white.template'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll need to retrieve some Amazon Resource Names (ARNs): think of these as codes that uniquely identify different resources on AWS.\n",
    "\n",
    "To find your `prehuman_arn`, ...\n",
    "\n",
    "To find your `acs_arn`, ...\n",
    "\n",
    "To find your `workteam_arn`, open the [AWS console](http://console.aws.amazon.com) and open SageMaker with the \"Find Services\" search bar. In the left-hand menu, click <kbd>Labeling workforces</kbd>.\n",
    "* If using a public workforce, find `Mechanical_turn_workforce_team` in the team summary, and copy the provided ARN.\n",
    "* If using a private workforce, click the <kbd>Private</kbd> tab at the top of the page, then scroll down to the list of private teams, and copy the ARN of the team you wish to use. Please refer to [this tutorial](docs/readme_privateWorkforce.md) to learn how to create and add users to your own private team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prehuman_arn = 'arn:aws:lambda:us-east-2:522254924758:function:serverlessrepo-aws-sagema-GtRecipePreHumanTaskFunc-xxxxxxxxxxxxx'\n",
    "acs_arn = 'arn:aws:lambda:us-east-1:919226420625:function:serverlessrepo-aws-sagema-GtRecipeAnnotationConsol-xxxxxxxxxxxxx'\n",
    "workteam_arn = 'arn:aws:sagemaker:us-east-2:394669845002:workteam/public-crowd/default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will package the task information. If you want to edit the number of annotations per image, the worker compensation, or the job description/keywords, do so here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about the data to be annotated and the annotation interface.\n",
    "task = {'BUCKET': BUCKET,         \n",
    "        'MANIFEST': MANIFEST, \n",
    "        'UITEMPLATE': UITEMPLATE, }\n",
    "\n",
    "# additional task info/instructions for annotators.\n",
    "task['info'] = {'description': 'Please label each body part of interest',\n",
    "                'keywords': ['pose', 'keypoints','animal'],\n",
    "                'title': 'Please label each body part of interest',\n",
    "                'job_name': '{}-'.format(task['BUCKET']) + str(int(time.time())), }\n",
    "\n",
    "# Number of annotators per frame, and how much each annotator is paid per frame.\n",
    "# Check AWS documentation for permissable compensation values. More workers will make your annotations\n",
    "# more robust to outliers, while lower compensation often leads to noisier pose annotations.\n",
    "task['price'] = {'num_workers': 5,\n",
    "                 'dollars': 0,\n",
    "                 'cents': 6,\n",
    "                 'tenthcent': 0, }\n",
    "\n",
    "# Insert ARNs for resources needed to run an image classification job.\n",
    "task['arns'] = {'prehuman_arn': prehuman_arn,\n",
    "                'acs_arn': acs_arn,\n",
    "                'workteam_arn': workteam_arn, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to submit the annotation job to SageMaker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SageMaker\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Make sure the bucket is in the same region as this notebook.\n",
    "mah.check_bucket_region(role, task)\n",
    "\n",
    "# Package information/settings for the human workforce.\n",
    "human_task_config = mah.configure_workforce(task)\n",
    "\n",
    "# Format the task request.\n",
    "ground_truth_request = mah.configure_workforce(task, human_task_config, role)\n",
    "\n",
    "# Submit the task request.\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_client.create_labeling_job(**ground_truth_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize manual pose annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile pose annotations for training MARS\n",
    "\n",
    "Now that you have collected some pose annotations, it's time to package them so you can use them to re-train MARS! To do so, we need to get your annotations into `tfrecord` format.\n",
    "\n",
    "MARS can also handle annotations created using other systems. To make MARS tfrecords from existing DeepLabCut training data, simply set `annotation_file` to be the path to the `.csv` file containing your manual annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '/path/to/human_annotations.manifest' # or you can use a csv from DeepLabCut\n",
    "keypoint_names = ['Nose','EarL','EarR','Neck','HipL','HipR','Tail']\n",
    "\n",
    "mat.create_tfrecords(annotation_file, image_directory, keypoint_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
