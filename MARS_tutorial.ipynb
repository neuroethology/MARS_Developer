{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full MARS demo\n",
    "\n",
    "This is the tutorial from the readme of MARS_Developer in Jupyter notebook form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìÅ Create a new MARS Training Project\n",
    "\n",
    "Your MARS project directory will contain all the files created during the process of training MARS detector and pose models on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download_MARS_checkpoints hasn't been implemented yet :(\n",
      "Project my_project created successfully.\n"
     ]
    }
   ],
   "source": [
    "from create_new_project import *\n",
    "\n",
    "location = 'K:/'\n",
    "name = 'my_project'\n",
    "\n",
    "create_new_project(location, name,\n",
    "                   download_MARS_checkpoints=True,\n",
    "                   download_demo_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**TODO** provide instructions on how to set up the fields of project_config.yaml.\n",
    "\n",
    "Also TODO: download model checkpoints (inception resnet at the very least, potentially MARS checkpoints as well- maybe make this an optional third argument to create_new_project.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ‚úçÔ∏è Collect a set of manually annotated animal poses\n",
    "In this step, we will get some training data for our detector and pose estimator.\n",
    "\n",
    "<font color=red>TODO: create a dummy dataset for users to download to try things out- maybe CRIM13?</font>\n",
    "\n",
    "> If you've already annotated some poses in DeepLabCut, copy the annotation csv to `my_project/annotation_data`, and the images you annotated to `my_project/annotation_data/raw_images`. Set `manifest_name` in `project_config.yaml` to the name of your csv.\n",
    ">\n",
    "> Then **skip to step 2.3** to format your data for training.\n",
    "\n",
    "### 2.1 Extract video frames that you would like to annotate\n",
    "First, we need to collect a set of video frames to annotate. The script `extract_frames.py` will sample frames from all videos found in a directory, and save those frames as jpg files.\n",
    "\n",
    "If you've already extracted frames, copy them to `my_project/annotation_data/raw_images` so they can be accessed during model training.\n",
    "\n",
    "<font color=red>TODO: wrap and test extract_raw_frames.py, and add the neuroethology Util repository to MARS_Developer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_raw_frames.py input_dir /path/to/videodir project /path/to/savedir/my_project n_frames 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a folder `my_project/annotation_data/raw_images` of video frames.\n",
    "\n",
    "### 2.2 Run a labeling job\n",
    "Refer to [these instructions](https://github.com/neuroethology/MARS_Developer/blob/develop/pose_annotation_tools/docs/readme_groundTruthSetup.md) to run a labeling job on Amazon SageMaker.\n",
    "\n",
    "This job will produce a file called `output.manifest` which you should add to `my_project/annotation_data`. If you change the name of this manifest file, be sure to update the `manifest_name` field of `my_project/project_config.yaml` appropriately.\n",
    "\n",
    "### 2.3 Post-process the manual pose annotations\n",
    "This script carries out several steps to create \"ground truth\" keypoint locations by taking the median across AWS workers, correct for left/right flips of body part labels, and package your data for model-training. The processed annotation data will be added as a new file to `annotation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_annotation_tools.annotation_postprocessing import *\n",
    "\n",
    "project_path = 'K:\\my_project'\n",
    "annotation_postprocessing(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize some manual pose annotations\n",
    "\n",
    "We'll next take a look at the inter-annotator variability in your pose labels, as this is a good predictor for how well MARS will perform. (If you annotated pose yourself you can skip this step.)\n",
    "\n",
    "<font color=red>TODO: consolidate visualization code and add it here.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéØ Fine-tune the MARS detector to your data\n",
    "\n",
    "<font color=red>TODO: set NUM_TRAIN_EXAMPLES in the train_config file programmatically! This affects learning rate scheduling, so we should get it right.\n",
    "\n",
    "Also TODO: clear out some of these warning messages.</font>\n",
    "\n",
    "### 3.1 Run the training code\n",
    "`run_training` takes two optional arguments:\n",
    "* `models` takes names of detectors to train, in case you only want to work on one detector at a time (MARS trains all detectors sequentially by default.)\n",
    "* `max_training_steps` overrides `NUM_TRAIN_ITERATIONS` in `train_config.yaml` (300,000 by default). You can set this to a small number to confirm everything is working before launching your full training job.\n",
    "\n",
    "If training is interrupted, it will resume from the most recent model checkpoint; MARS saves checkpoints every 30 minutes (adjust this in `my_project/detection/train_config.yaml`), and when training completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ann\\Documents\\GitHub\\MARS_Developer\\multibox_detection\\train.py:345: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  bbox_priors = np.array(bbox_priors).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\my_project\\detection\\black_top_log\\model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path D:\\my_project\\detection\\black_top_log\\model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 10.\n",
      "INFO:tensorflow:global_step/sec: 0.0374452\n",
      "INFO:tensorflow:Recording summary at step 11.\n",
      "INFO:tensorflow:global step 11: loss = 548.1651 (37.459 sec/step)\n",
      "INFO:tensorflow:global step 12: loss = 550.7853 (7.828 sec/step)\n",
      "INFO:tensorflow:global step 13: loss = 547.7502 (6.653 sec/step)\n",
      "INFO:tensorflow:global step 14: loss = 427.0122 (6.828 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.0999022\n",
      "INFO:tensorflow:Recording summary at step 14.\n",
      "INFO:tensorflow:global step 15: loss = 607.3123 (5.613 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ann\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\summary\\writer\\writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
      "  warnings.warn(\"Attempting to use a closed FileWriter. \"\n",
      "C:\\Users\\Ann\\Documents\\GitHub\\MARS_Developer\\multibox_detection\\train.py:345: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  bbox_priors = np.array(bbox_priors).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\my_project\\detection\\white_top_log\\model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path D:\\my_project\\detection\\white_top_log\\model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 10.\n",
      "INFO:tensorflow:Recording summary at step 11.\n",
      "INFO:tensorflow:global step 11: loss = 554.7497 (38.983 sec/step)\n",
      "INFO:tensorflow:global step 12: loss = 463.0262 (6.502 sec/step)\n",
      "INFO:tensorflow:global step 13: loss = 585.4284 (7.020 sec/step)\n",
      "INFO:tensorflow:global step 14: loss = 589.9485 (6.977 sec/step)\n",
      "INFO:tensorflow:Recording summary at step 14.\n",
      "INFO:tensorflow:global step 15: loss = 592.4981 (5.703 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ann\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\summary\\writer\\writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
      "  warnings.warn(\"Attempting to use a closed FileWriter. \"\n"
     ]
    }
   ],
   "source": [
    "from multibox_detection import train\n",
    "\n",
    "project_path = 'K:\\my_project'\n",
    "train.run_training(project_path, max_training_steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize model performance and pick the best checkpoint\n",
    "\n",
    "During training, we saved a model checkpoint every 30 minutes (by default). We'll visualize the performance of our saved checkpoints on a held-out validation set of images to determine whether performance has plateaued, and use this to pick our best-performing version of the model.\n",
    "\n",
    "<font color=red>TODO: add early stopping code here?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After picking the checkpoint that performs best on your validation set, we can create some additional performance figures on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code to generate PR curve figure here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 (Optional) test your detector on new videos\n",
    "\n",
    "üí° If you are working with top-view mouse videos, it is possible that MARS's pose estimator will work for you out of the box now that you have a working detector. Skip to step 5 to try it out! If you're not happy with the performance, continue to step 4 where you'll be able to fine-tune the pose estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üêÅ Fine-tune the MARS pose estimator to your data\n",
    "\n",
    "<font color=red>TODO: write run_training function for pose model</font>\n",
    "\n",
    "### 4.1 Run the training code\n",
    "Training the pose estimator is the same as training your detectors.\n",
    "\n",
    "`run_training` again takes two optional arguments:\n",
    "* `models` takes names of pose models to train, in case you only want to work on one model at a time (MARS trains all detectors sequentially by default.)\n",
    "* `max_training_steps` overrides `NUM_TRAIN_ITERATIONS` in `train_config.yaml`. You can set this to a small number to confirm everything is working before launching your full training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hourglass_pose import train\n",
    "\n",
    "project_path = 'K:\\my_project'\n",
    "train.run_training(project_path, max_training_steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualize model performance and pick the best checkpoint\n",
    "\n",
    "As for the detector, we'll visualize the performance of our saved checkpoints to determine whether performance has plateaued and to pick our best-performing version of the pose model.\n",
    "\n",
    "<font color=red>TODO: add early stopping code here?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üöÄ Deploy your new detection and pose models\n",
    "\n",
    "<font color=red>TODO: add process to save protobuf and include instructions for how to add it to MARS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üí™ Train new behavior classifiers\n",
    "\n",
    "<font color=red>TODO: add MARS_train_infer to repo!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars",
   "language": "python",
   "name": "mars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
