{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full MARS demo\n",
    "\n",
    "This is the tutorial from the readme of MARS_Developer in Jupyter notebook form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìÅ Create a new MARS Training Project\n",
    "\n",
    "Your MARS project directory will contain all the files created during the process of training MARS detector and pose models on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project my_project created successfully.\n"
     ]
    }
   ],
   "source": [
    "from create_new_project import *\n",
    "# TODO: find less clunky way to call these scripts from Jupyter\n",
    "\n",
    "location = 'D:/'\n",
    "name = 'my_project'\n",
    "\n",
    "create_new_project(location, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**TODO** provide instructions on how to set up the fields of project_config.yaml.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ‚úçÔ∏è Collect a set of manually annotated animal poses.\n",
    "In this step, we will get some training data for our detector and pose estimator.\n",
    "\n",
    "> If you've already annotated some poses yourself, eg in DeepLabCut, copy that annotation csv to `my_project/annotation_data` and the images you annotated to `my_project/annotation_data/raw_images`, and set `manifest_name` in `project_config.yaml` to the name of your csv. Then proceed to **step 2.3**.\n",
    "\n",
    "### 2.1 Extract video frames that you would like to annotate.\n",
    "First, we need to collect a set of video frames to annotate. The script `extract_frames.py` will sample frames from all videos found in a directory, and save those frames as jpg files.\n",
    "\n",
    "<font color=red>TODO: wrap and test this; add Util to MARS_Developer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_raw_frames.py input_dir /path/to/videodir project /path/to/savedir/my_project n_frames 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a folder `my_project/annotation_data` containing a directory `raw_images` of video frames.\n",
    "\n",
    "### 2.2 Run a labeling job.\n",
    "Refer to [these instructions](https://github.com/neuroethology/MARS_Developer/blob/develop/pose_annotation_tools/docs/readme_groundTruthSetup.md) to run a labeling job on Amazon SageMaker.\n",
    "\n",
    "This job will produce a file called `output.manifest` which you should add to `my_project/annotation_data`. If you change the name of this manifest file, be sure to update the `manifest_name` field of `my_project/project_config.yaml` appropriately.\n",
    "\n",
    "### 2.3 Post-process the manual pose annotations.\n",
    "This script carries out several steps to create \"ground truth\" keypoint locations by taking the median across AWS workers, correct for left/right flips of body part labels, and package your data for model-training. The processed annotation data will be added as a new file to `annotation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ann\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Generating black_top priors...\n",
      "WARNING:tensorflow:From C:\\Users\\Ann\\Documents\\GitHub\\MARS_Developer\\pose_annotation_tools\\priors_generator.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "done.\n",
      "Generating white_top priors...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from pose_annotation_tools.annotation_postprocessing import *\n",
    "\n",
    "project_path = 'D:\\my_project'\n",
    "annotation_postprocessing(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize some manual pose annotations.\n",
    "\n",
    "We'll next take a look at the inter-annotator variability in your pose labels, as this is a good predictor for how well MARS will perform. (If you annotated pose yourself you can skip this step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéØ Fine-tune the MARS detector to your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars",
   "language": "python",
   "name": "mars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
